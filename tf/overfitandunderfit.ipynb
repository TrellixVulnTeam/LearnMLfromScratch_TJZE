{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索过拟合和欠拟合\n",
    "\n",
    "与往常一样，此示例中的代码将使用 tf.keras API；如需了解详情，请参阅 TensorFlow Keras 指南。\n",
    "\n",
    "在之前的两个示例（分类影评和预测房价）中，我们了解到在训练周期达到一定次数后，模型在验证数据上的准确率会达到峰值，然后便开始下降。\n",
    "\n",
    "也就是说，模型会过拟合训练数据。请务必学习如何处理过拟合。尽管通常可以在训练集上实现很高的准确率，但我们真正想要的是开发出能够很好地泛化到测试数据（或之前未见过的数据）的模型。\n",
    "\n",
    "与过拟合相对的是欠拟合。当测试数据仍存在改进空间时，便会发生欠拟合。出现这种情况的原因有很多：模型不够强大、过于正则化，或者根本没有训练足够长的时间。这意味着网络未学习训练数据中的相关模式。\n",
    "\n",
    "如果训练时间过长，模型将开始过拟合，并从训练数据中学习无法泛化到测试数据的模式。我们需要在这两者之间实现平衡。了解如何训练适当的周期次数是一项很实用的技能，接下来我们将介绍这一技能。\n",
    "\n",
    "为了防止发生过拟合，最好的解决方案是使用更多训练数据。用更多数据进行训练的模型自然能够更好地泛化。如无法采用这种解决方案，则次优解决方案是使用正则化等技术。这些技术会限制模型可以存储的信息的数量和类型。如果网络只能记住少量模式，那么优化过程将迫使它专注于最突出的模式，因为这些模式更有机会更好地泛化。\n",
    "\n",
    "在此笔记本中，我们将探索两种常见的正则化技术（权重正则化和丢弃），并使用它们改进我们的 IMDB 影评分类笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 IMDB 数据集\n",
    "\n",
    "我们不会像在上一个笔记本中那样使用嵌入，而是对句子进行多热编码。该模型将很快过拟合训练集。它将用来演示何时发生过拟合，以及如何防止过拟合。\n",
    "\n",
    "对列表进行多热编码意味着将它们转换为由 0 和 1 组成的向量。例如，将序列 [3, 5] 转换为一个 10000 维的向量（除索引 3 和 5 转换为 1 之外，其余全为 0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
    "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看生成的其中一个多热向量。字词索引按频率排序，因此索引 0 附近应该有更多的 1 值，如下图所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b9d0e48>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演示过拟合\n",
    "\n",
    "要防止过拟合，最简单的方法是缩小模型，即减少模型中可学习参数的数量（由层数和每层的单元数决定）。在深度学习中，模型中可学习参数的数量通常称为模型的“容量”。直观而言，参数越多的模型“记忆容量”越大，因此能够轻松学习训练样本与其目标之间的字典式完美映射（无任何泛化能力的映射），但如果要对之前未见过的数据做出预测，这种映射毫无用处。\n",
    "\n",
    "请务必谨记：深度学习模型往往善于与训练数据拟合，但真正的挑战是泛化，而非拟合。\n",
    "\n",
    "另一方面，如果网络的记忆资源有限，便无法轻松学习映射。为了最小化损失，它必须学习具有更强预测能力的压缩表示法。同时，如果模型太小，它将难以与训练数据拟合。我们需要在“太多容量”和“容量不足”这两者之间实现平衡。\n",
    "\n",
    "遗憾的是，并没有什么神奇公式可用来确定合适的模型大小或架构（由层数或每层的合适大小决定）。您将需要尝试一系列不同的架构。\n",
    "\n",
    "要找到合适的模型大小，最好先使用相对较少的层和参数，然后开始增加层的大小或添加新的层，直到看到返回的验证损失不断减小为止。我们在影评分类网络上试试这个方法。\n",
    "\n",
    "我们将仅使用 Dense 层创建一个简单的基准模型，然后创建更小和更大的版本，并比较这些版本。\n",
    "\n",
    "### 创建基准模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = keras.Sequential([\n",
    "    # `input_shape` is only required here so that `.summary` works.\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "baseline_model.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 0.5114 - acc: 0.7901 - binary_crossentropy: 0.5114 - val_loss: 0.3544 - val_acc: 0.8728 - val_binary_crossentropy: 0.3544\n",
      "Epoch 2/20\n",
      " - 4s - loss: 0.2613 - acc: 0.9102 - binary_crossentropy: 0.2613 - val_loss: 0.2880 - val_acc: 0.8871 - val_binary_crossentropy: 0.2880\n",
      "Epoch 3/20\n",
      " - 4s - loss: 0.1899 - acc: 0.9347 - binary_crossentropy: 0.1899 - val_loss: 0.2869 - val_acc: 0.8864 - val_binary_crossentropy: 0.2869\n",
      "Epoch 4/20\n",
      " - 4s - loss: 0.1505 - acc: 0.9497 - binary_crossentropy: 0.1505 - val_loss: 0.3068 - val_acc: 0.8806 - val_binary_crossentropy: 0.3068\n",
      "Epoch 5/20\n",
      " - 4s - loss: 0.1223 - acc: 0.9612 - binary_crossentropy: 0.1223 - val_loss: 0.3286 - val_acc: 0.8770 - val_binary_crossentropy: 0.3286\n",
      "Epoch 6/20\n",
      " - 4s - loss: 0.0984 - acc: 0.9716 - binary_crossentropy: 0.0984 - val_loss: 0.3547 - val_acc: 0.8738 - val_binary_crossentropy: 0.3547\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.0783 - acc: 0.9792 - binary_crossentropy: 0.0783 - val_loss: 0.3963 - val_acc: 0.8661 - val_binary_crossentropy: 0.3963\n",
      "Epoch 8/20\n",
      " - 4s - loss: 0.0619 - acc: 0.9858 - binary_crossentropy: 0.0619 - val_loss: 0.4200 - val_acc: 0.8660 - val_binary_crossentropy: 0.4200\n",
      "Epoch 9/20\n",
      " - 4s - loss: 0.0473 - acc: 0.9908 - binary_crossentropy: 0.0473 - val_loss: 0.4546 - val_acc: 0.8639 - val_binary_crossentropy: 0.4546\n",
      "Epoch 10/20\n",
      " - 4s - loss: 0.0353 - acc: 0.9942 - binary_crossentropy: 0.0353 - val_loss: 0.4873 - val_acc: 0.8629 - val_binary_crossentropy: 0.4873\n",
      "Epoch 11/20\n",
      " - 4s - loss: 0.0267 - acc: 0.9966 - binary_crossentropy: 0.0267 - val_loss: 0.5205 - val_acc: 0.8622 - val_binary_crossentropy: 0.5205\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.0199 - acc: 0.9976 - binary_crossentropy: 0.0199 - val_loss: 0.5589 - val_acc: 0.8604 - val_binary_crossentropy: 0.5589\n",
      "Epoch 13/20\n",
      " - 4s - loss: 0.0143 - acc: 0.9983 - binary_crossentropy: 0.0143 - val_loss: 0.5929 - val_acc: 0.8587 - val_binary_crossentropy: 0.5929\n",
      "Epoch 14/20\n",
      " - 4s - loss: 0.0103 - acc: 0.9990 - binary_crossentropy: 0.0103 - val_loss: 0.6244 - val_acc: 0.8578 - val_binary_crossentropy: 0.6244\n",
      "Epoch 15/20\n",
      " - 4s - loss: 0.0076 - acc: 0.9998 - binary_crossentropy: 0.0076 - val_loss: 0.6558 - val_acc: 0.8573 - val_binary_crossentropy: 0.6558\n",
      "Epoch 16/20\n",
      " - 4s - loss: 0.0057 - acc: 0.9999 - binary_crossentropy: 0.0057 - val_loss: 0.6881 - val_acc: 0.8574 - val_binary_crossentropy: 0.6881\n",
      "Epoch 17/20\n",
      " - 4s - loss: 0.0045 - acc: 0.9999 - binary_crossentropy: 0.0045 - val_loss: 0.7121 - val_acc: 0.8573 - val_binary_crossentropy: 0.7121\n",
      "Epoch 18/20\n",
      " - 4s - loss: 0.0036 - acc: 1.0000 - binary_crossentropy: 0.0036 - val_loss: 0.7369 - val_acc: 0.8568 - val_binary_crossentropy: 0.7369\n",
      "Epoch 19/20\n",
      " - 4s - loss: 0.0029 - acc: 1.0000 - binary_crossentropy: 0.0029 - val_loss: 0.7597 - val_acc: 0.8566 - val_binary_crossentropy: 0.7597\n",
      "Epoch 20/20\n",
      " - 4s - loss: 0.0024 - acc: 1.0000 - binary_crossentropy: 0.0024 - val_loss: 0.7795 - val_acc: 0.8561 - val_binary_crossentropy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "baseline_history = baseline_model.fit(train_data,\n",
    "                                      train_labels,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(test_data, test_labels),\n",
    "                                      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个更小的模型\n",
    "我们创建一个隐藏单元更少的模型，然后与我们刚刚创建的基准模型进行比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 4)                 40004     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 40,029\n",
      "Trainable params: 40,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smaller_model = keras.Sequential([\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "smaller_model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "smaller_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 0.5285 - acc: 0.7803 - binary_crossentropy: 0.5285 - val_loss: 0.3975 - val_acc: 0.8645 - val_binary_crossentropy: 0.3975\n",
      "Epoch 2/20\n",
      " - 4s - loss: 0.3120 - acc: 0.8993 - binary_crossentropy: 0.3120 - val_loss: 0.3196 - val_acc: 0.8810 - val_binary_crossentropy: 0.3196\n",
      "Epoch 3/20\n",
      " - 4s - loss: 0.2412 - acc: 0.9203 - binary_crossentropy: 0.2412 - val_loss: 0.2925 - val_acc: 0.8861 - val_binary_crossentropy: 0.2925\n",
      "Epoch 4/20\n",
      " - 4s - loss: 0.2019 - acc: 0.9342 - binary_crossentropy: 0.2019 - val_loss: 0.2865 - val_acc: 0.8852 - val_binary_crossentropy: 0.2865\n",
      "Epoch 5/20\n",
      " - 4s - loss: 0.1754 - acc: 0.9440 - binary_crossentropy: 0.1754 - val_loss: 0.2892 - val_acc: 0.8832 - val_binary_crossentropy: 0.2892\n",
      "Epoch 6/20\n",
      " - 4s - loss: 0.1550 - acc: 0.9502 - binary_crossentropy: 0.1550 - val_loss: 0.2922 - val_acc: 0.8828 - val_binary_crossentropy: 0.2922\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.1386 - acc: 0.9574 - binary_crossentropy: 0.1386 - val_loss: 0.3010 - val_acc: 0.8807 - val_binary_crossentropy: 0.3010\n",
      "Epoch 8/20\n",
      " - 4s - loss: 0.1252 - acc: 0.9624 - binary_crossentropy: 0.1252 - val_loss: 0.3132 - val_acc: 0.8773 - val_binary_crossentropy: 0.3132\n",
      "Epoch 9/20\n",
      " - 4s - loss: 0.1142 - acc: 0.9650 - binary_crossentropy: 0.1142 - val_loss: 0.3259 - val_acc: 0.8754 - val_binary_crossentropy: 0.3259\n",
      "Epoch 10/20\n",
      " - 4s - loss: 0.1036 - acc: 0.9701 - binary_crossentropy: 0.1036 - val_loss: 0.3397 - val_acc: 0.8724 - val_binary_crossentropy: 0.3397\n",
      "Epoch 11/20\n",
      " - 4s - loss: 0.0947 - acc: 0.9736 - binary_crossentropy: 0.0947 - val_loss: 0.3546 - val_acc: 0.8708 - val_binary_crossentropy: 0.3546\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.0859 - acc: 0.9764 - binary_crossentropy: 0.0859 - val_loss: 0.3706 - val_acc: 0.8686 - val_binary_crossentropy: 0.3706\n",
      "Epoch 13/20\n",
      " - 4s - loss: 0.0783 - acc: 0.9800 - binary_crossentropy: 0.0783 - val_loss: 0.3862 - val_acc: 0.8668 - val_binary_crossentropy: 0.3862\n",
      "Epoch 14/20\n",
      " - 4s - loss: 0.0716 - acc: 0.9828 - binary_crossentropy: 0.0716 - val_loss: 0.4057 - val_acc: 0.8648 - val_binary_crossentropy: 0.4057\n",
      "Epoch 15/20\n",
      " - 4s - loss: 0.0652 - acc: 0.9854 - binary_crossentropy: 0.0652 - val_loss: 0.4205 - val_acc: 0.8643 - val_binary_crossentropy: 0.4205\n",
      "Epoch 16/20\n",
      " - 4s - loss: 0.0597 - acc: 0.9874 - binary_crossentropy: 0.0597 - val_loss: 0.4392 - val_acc: 0.8627 - val_binary_crossentropy: 0.4392\n",
      "Epoch 17/20\n",
      " - 4s - loss: 0.0547 - acc: 0.9896 - binary_crossentropy: 0.0547 - val_loss: 0.4588 - val_acc: 0.8607 - val_binary_crossentropy: 0.4588\n",
      "Epoch 18/20\n",
      " - 4s - loss: 0.0496 - acc: 0.9910 - binary_crossentropy: 0.0496 - val_loss: 0.4862 - val_acc: 0.8589 - val_binary_crossentropy: 0.4862\n",
      "Epoch 19/20\n",
      " - 4s - loss: 0.0450 - acc: 0.9922 - binary_crossentropy: 0.0450 - val_loss: 0.4982 - val_acc: 0.8580 - val_binary_crossentropy: 0.4982\n",
      "Epoch 20/20\n",
      " - 4s - loss: 0.0410 - acc: 0.9935 - binary_crossentropy: 0.0410 - val_loss: 0.5183 - val_acc: 0.8571 - val_binary_crossentropy: 0.5183\n"
     ]
    }
   ],
   "source": [
    "smaller_history = smaller_model.fit(train_data,\n",
    "                                    train_labels,\n",
    "                                    epochs=20,\n",
    "                                    batch_size=512,\n",
    "                                    validation_data=(test_data, test_labels),\n",
    "                                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个更大的模型\n",
    "\n",
    "作为练习，您可以创建一个更大的模型，看看它多快开始过拟合。接下来，我们向这个基准添加一个容量大得多的网络，远远超出解决问题所需的容量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 5,383,681\n",
      "Trainable params: 5,383,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bigger_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "bigger_model.compile(optimizer='adam',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy','binary_crossentropy'])\n",
    "\n",
    "bigger_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 14s - loss: 0.3485 - acc: 0.8529 - binary_crossentropy: 0.3485 - val_loss: 0.2989 - val_acc: 0.8777 - val_binary_crossentropy: 0.2989\n",
      "Epoch 2/20\n",
      " - 13s - loss: 0.1465 - acc: 0.9459 - binary_crossentropy: 0.1465 - val_loss: 0.3549 - val_acc: 0.8662 - val_binary_crossentropy: 0.3549\n",
      "Epoch 3/20\n",
      " - 13s - loss: 0.0534 - acc: 0.9844 - binary_crossentropy: 0.0534 - val_loss: 0.4238 - val_acc: 0.8671 - val_binary_crossentropy: 0.4238\n",
      "Epoch 4/20\n",
      " - 13s - loss: 0.0094 - acc: 0.9986 - binary_crossentropy: 0.0094 - val_loss: 0.5666 - val_acc: 0.8692 - val_binary_crossentropy: 0.5666\n",
      "Epoch 5/20\n",
      " - 13s - loss: 0.0013 - acc: 1.0000 - binary_crossentropy: 0.0013 - val_loss: 0.6656 - val_acc: 0.8692 - val_binary_crossentropy: 0.6656\n",
      "Epoch 6/20\n",
      " - 13s - loss: 8.6223e-04 - acc: 1.0000 - binary_crossentropy: 8.6223e-04 - val_loss: 0.6981 - val_acc: 0.8694 - val_binary_crossentropy: 0.6981\n",
      "Epoch 7/20\n",
      " - 13s - loss: 1.9428e-04 - acc: 1.0000 - binary_crossentropy: 1.9428e-04 - val_loss: 0.7230 - val_acc: 0.8702 - val_binary_crossentropy: 0.7230\n",
      "Epoch 8/20\n",
      " - 13s - loss: 1.3079e-04 - acc: 1.0000 - binary_crossentropy: 1.3079e-04 - val_loss: 0.7452 - val_acc: 0.8704 - val_binary_crossentropy: 0.7452\n",
      "Epoch 9/20\n",
      " - 13s - loss: 9.7826e-05 - acc: 1.0000 - binary_crossentropy: 9.7826e-05 - val_loss: 0.7631 - val_acc: 0.8703 - val_binary_crossentropy: 0.7631\n",
      "Epoch 10/20\n",
      " - 13s - loss: 7.6754e-05 - acc: 1.0000 - binary_crossentropy: 7.6754e-05 - val_loss: 0.7769 - val_acc: 0.8707 - val_binary_crossentropy: 0.7769\n",
      "Epoch 11/20\n",
      " - 13s - loss: 6.2014e-05 - acc: 1.0000 - binary_crossentropy: 6.2014e-05 - val_loss: 0.7902 - val_acc: 0.8704 - val_binary_crossentropy: 0.7902\n",
      "Epoch 12/20\n",
      " - 13s - loss: 5.1093e-05 - acc: 1.0000 - binary_crossentropy: 5.1093e-05 - val_loss: 0.8018 - val_acc: 0.8703 - val_binary_crossentropy: 0.8018\n",
      "Epoch 13/20\n",
      " - 13s - loss: 4.2858e-05 - acc: 1.0000 - binary_crossentropy: 4.2858e-05 - val_loss: 0.8116 - val_acc: 0.8704 - val_binary_crossentropy: 0.8116\n",
      "Epoch 14/20\n",
      " - 13s - loss: 3.6464e-05 - acc: 1.0000 - binary_crossentropy: 3.6464e-05 - val_loss: 0.8209 - val_acc: 0.8703 - val_binary_crossentropy: 0.8209\n",
      "Epoch 15/20\n",
      " - 13s - loss: 3.1316e-05 - acc: 1.0000 - binary_crossentropy: 3.1316e-05 - val_loss: 0.8297 - val_acc: 0.8702 - val_binary_crossentropy: 0.8297\n",
      "Epoch 16/20\n",
      " - 13s - loss: 2.7184e-05 - acc: 1.0000 - binary_crossentropy: 2.7184e-05 - val_loss: 0.8372 - val_acc: 0.8705 - val_binary_crossentropy: 0.8372\n",
      "Epoch 17/20\n",
      " - 13s - loss: 2.3793e-05 - acc: 1.0000 - binary_crossentropy: 2.3793e-05 - val_loss: 0.8455 - val_acc: 0.8701 - val_binary_crossentropy: 0.8455\n",
      "Epoch 18/20\n",
      " - 12s - loss: 2.0981e-05 - acc: 1.0000 - binary_crossentropy: 2.0981e-05 - val_loss: 0.8518 - val_acc: 0.8703 - val_binary_crossentropy: 0.8518\n",
      "Epoch 19/20\n",
      " - 12s - loss: 1.8601e-05 - acc: 1.0000 - binary_crossentropy: 1.8601e-05 - val_loss: 0.8587 - val_acc: 0.8702 - val_binary_crossentropy: 0.8587\n",
      "Epoch 20/20\n",
      " - 12s - loss: 1.6600e-05 - acc: 1.0000 - binary_crossentropy: 1.6600e-05 - val_loss: 0.8655 - val_acc: 0.8702 - val_binary_crossentropy: 0.8655\n"
     ]
    }
   ],
   "source": [
    "# 再次使用相同的数据训练该模型：\n",
    "bigger_history = bigger_model.fit(train_data, train_labels,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(test_data, test_labels),\n",
    "                                  verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制训练损失和验证损失图表\n",
    "实线表示训练损失，虚线表示验证损失（请谨记：验证损失越低，表示模型越好）。在此示例中，较小的网络开始过拟合的时间比基准模型晚（前者在 6 个周期之后，后者在 4 个周期之后），并且开始过拟合后，它的效果下降速度也慢得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories, key='binary_crossentropy'):\n",
    "  plt.figure(figsize=(16,10))\n",
    "\n",
    "  for name, history in histories:\n",
    "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(key.replace('_',' ').title())\n",
    "  plt.legend()\n",
    "\n",
    "  plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "plot_history([('baseline', baseline_history),\n",
    "              ('smaller', smaller_history),\n",
    "              ('bigger', bigger_history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，较大的网络几乎仅仅 1 个周期之后便立即开始过拟合，并且之后严重得多。网络容量越大，便能够越快对训练数据进行建模（产生较低的训练损失），但越容易过拟合（导致训练损失与验证损失之间的差异很大）。\n",
    "\n",
    "## 策略\n",
    "### 添加权重正则化\n",
    "您可能熟悉奥卡姆剃刀定律：如果对于同一现象有两种解释，最可能正确的解释是“最简单”的解释，即做出最少量假设的解释。这也适用于神经网络学习的模型：给定一些训练数据和一个网络架构，有多组权重值（多个模型）可以解释数据，而简单模型比复杂模型更不容易过拟合。\n",
    "\n",
    "在这种情况下，“简单模型”是一种参数值分布的熵较低的模型（或者具有较少参数的模型，如我们在上面的部分中所见）。因此，要缓解过拟合，一种常见方法是限制网络的复杂性，具体方法是强制要求其权重仅采用较小的值，使权重值的分布更“规则”。这称为“权重正则化”，通过向网络的损失函数添加与权重较大相关的代价来实现。这个代价分为两种类型：\n",
    "\n",
    "- L1 正则化，其中所添加的代价与权重系数的绝对值（即所谓的权重“L1 范数”）成正比。\n",
    "\n",
    "- L2 正则化，其中所添加的代价与权重系数值的平方（即所谓的权重“L2 范数”）成正比。L2 正则化在神经网络领域也称为权重衰减。不要因为名称不同而感到困惑：从数学角度来讲，权重衰减与 L2 正则化完全相同。\n",
    "\n",
    "在 tf.keras 中，权重正则化的添加方法如下：将权重正则化实例作为关键字参数传递给层。现在，我们来添加 L2 权重正则化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                       activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                       activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "l2_model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "l2_model_history = l2_model.fit(train_data, train_labels,\n",
    "                                epochs=20,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(test_data, test_labels),\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2(0.001) 表示层的权重矩阵中的每个系数都会将 0.001 * weight_coefficient_value**2 添加到网络的总损失中。请注意，由于此惩罚仅在训练时添加，此网络在训练时的损失将远高于测试时。\n",
    "\n",
    "以下是 L2 正则化惩罚的影响："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('baseline', baseline_history),\n",
    "              ('l2', l2_model_history)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，L2 正则化模型的过拟合抵抗能力比基准模型强得多，虽然这两个模型的参数数量相同。\n",
    "\n",
    "添加丢弃层\n",
    "丢弃是由 Hinton 及其在多伦多大学的学生开发的，是最有效且最常用的神经网络正则化技术之一。丢弃（应用于某个层）是指在训练期间随机“丢弃”（即设置为 0）该层的多个输出特征。假设某个指定的层通常会在训练期间针对给定的输入样本返回一个向量 [0.2, 0.5, 1.3, 0.8, 1.1]；在应用丢弃后，此向量将随机分布几个 0 条目，例如 [0, 0.5, 1.3, 0, 1.1]。“丢弃率”指变为 0 的特征所占的比例，通常设置在 0.2 和 0.5 之间。在测试时，网络不会丢弃任何单元，而是将层的输出值按等同于丢弃率的比例进行缩减，以便平衡以下事实：测试时的活跃单元数大于训练时的活跃单元数。\n",
    "\n",
    "在 tf.keras 中，您可以通过丢弃层将丢弃引入网络中，以便事先将其应用于层的输出。\n",
    "\n",
    "下面我们在 IMDB 网络中添加两个丢弃层，看看它们在降低过拟合方面表现如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "dpt_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy','binary_crossentropy'])\n",
    "\n",
    "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(test_data, test_labels),\n",
    "                                  verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('baseline', baseline_history),\n",
    "              ('dropout', dpt_model_history)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加丢弃层可明显改善基准模型。\n",
    "\n",
    "下面总结一下防止神经网络出现过拟合的最常见方法：\n",
    "\n",
    "- 获取更多训练数据。\n",
    "- 降低网络容量。\n",
    "- 添加权重正则化。\n",
    "- 添加丢弃层。\n",
    "还有两个重要的方法在本指南中没有介绍：数据增强和批次标准化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
